{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Libraries\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install -U scikit-learn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4b92a",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzvf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc600e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch_1 = unpickle(r'cifar-10-batches-py/data_batch_1')\n",
    "print(type(data_batch_1))\n",
    "print(\"--------------------------\")\n",
    "print(data_batch_1.keys())\n",
    "print(\"--------------------------\")\n",
    "for item in data_batch_1:\n",
    "    print(item, type(data_batch_1[item]))\n",
    "print(\"--------------------------\")\n",
    "print(\"Labels:\", set(data_batch_1['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "batch_1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "batch_2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "batch_3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "batch_4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "batch_5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "test_batch =unpickle('cifar-10-batches-py/test_batch')\n",
    "\n",
    "X_train_pixel = np.concatenate([batch_1['data'], batch_2['data'], batch_3['data'], batch_4['data'], batch_5['data']], axis=0)\n",
    "y_train_pixel = np.concatenate([batch_1['labels'], batch_2['labels'], batch_3['labels'], batch_4['labels'], batch_5['labels']], axis=0)\n",
    "\n",
    "X_test_pixel = test_batch['data']\n",
    "y_test_pixel = test_batch['labels']\n",
    "\n",
    "# Reshape the image data to its original dimensions\n",
    "X_train_pixel = X_train_pixel.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "X_test_pixel = X_test_pixel.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "print(f'Training Samples: {len(X_train_pixel)}')\n",
    "print(f'Testing Samples: {len(X_test_pixel)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pixel[0].shape)\n",
    "print(X_test_pixel[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc656cc",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Converting to grayscale\n",
    "X_training_gray = [ rgb2gray(np.array(X_train_pixel[i])) for i in range(50000)]\n",
    "X_testing_gray  = [ rgb2gray(np.array(X_test_pixel[i])) for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92889b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "num_images = 10\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(num_images):\n",
    "    # RGB image\n",
    "    plt.subplot(2, num_images, i + 1)\n",
    "    plt.imshow(X_train_pixel[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # grayscale image\n",
    "    plt.subplot(2, num_images, i + num_images + 1)\n",
    "    plt.imshow(X_training_gray[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "print(\"RGB along with corresponding grayscaled image\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b801710",
   "metadata": {},
   "source": [
    "## Feature Extraction (HoGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orientations = 9\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (2, 2)\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    # Compute HoG features\n",
    "    hog_features, hog_image = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                                   cells_per_block=cells_per_block, visualize=True)\n",
    "\n",
    "    # Rescale histogram for better visualization\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    return hog_features, hog_image_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef89aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_HoG = []\n",
    "X_train_HoG_rescaled = []\n",
    "y_training = np.array(y_train_pixel)\n",
    "\n",
    "for image in X_training_gray:\n",
    "    hog_features, hog_image = extract_hog_features(image)\n",
    "    X_train_HoG.append(hog_features)\n",
    "    X_train_HoG_rescaled.append(hog_image)\n",
    "\n",
    "X_training = np.array(X_train_HoG)\n",
    "X_test_HoG = []\n",
    "X_test_HoG_rescaled = []\n",
    "y_testing = np.array(y_test_pixel)\n",
    "\n",
    "for image in X_testing_gray:\n",
    "    hog_features, hog_image = extract_hog_features(image)\n",
    "    X_test_HoG.append(hog_features)\n",
    "    X_test_HoG_rescaled.append(hog_image)\n",
    "X_testing = np.array(X_test_HoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffede463",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('X_testing_HoG_.npz', X_testing)\n",
    "np.savez_compressed('X_training_HoG_.npz', X_training)\n",
    "np.savez_compressed('y_training_.npz', y_training)\n",
    "np.savez_compressed('y_testing_.npz', y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "\n",
    "plt.figure(figsize=(16, 5))  # Adjust the figure size to accommodate three images per row\n",
    "\n",
    "for i in range(num_images):\n",
    "    # RGB image\n",
    "    plt.subplot(3, num_images, i + 1)\n",
    "    plt.imshow(X_train_pixel[i])\n",
    "    plt.title('RGB')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Grayscale image\n",
    "    plt.subplot(3, num_images, i + 1 + num_images)\n",
    "    plt.imshow(X_training_gray[i], cmap='gray')\n",
    "    plt.title('Grayscale')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # HoG visualization\n",
    "    plt.subplot(3, num_images, i + 1 + 2 * num_images)\n",
    "    # Assuming X_train_HoG_rescaled contains the rescaled HoG images\n",
    "    plt.imshow(X_train_HoG_rescaled[i], cmap='gray')\n",
    "    plt.title('HoG')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba146f9d",
   "metadata": {},
   "source": [
    "## Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'X_training shape is {}'.format( X_training.shape) )\n",
    "print( 'y_training shape is {}'.format( y_training.shape ) )\n",
    "print( 'X_testing shape is {}'.format( X_testing.shape ) )\n",
    "print( 'y_testing shape is {}'.format( y_testing.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Overview of Training Data (Features)')\n",
    "pd.DataFrame( X_training ).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print( 'Overview of Training Data (Target)')\n",
    "pd.DataFrame( y_training ).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dee61",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "X_training_mean = np.mean(X_training, axis=0)  # Compute mean along each feature\n",
    "X_training_std = np.std(X_training, axis=0)    # Compute standard deviation along each feature\n",
    "X_training_scaled = (X_training - X_training_mean) / X_training_std\n",
    "\n",
    "# Standardize the testing data using mean and std computed from training data\n",
    "X_testing_scaled = (X_testing - X_training_mean) / X_training_std\n",
    "\n",
    "# After standardization\n",
    "print('X_training_scaled shape is {}'.format(X_training_scaled.shape))\n",
    "print('X_testing_scaled shape is {}'.format(X_testing_scaled.shape))\n",
    "\n",
    "print('X_training_scaled after standardization:')\n",
    "print(X_training_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_training_scaled, rowvar=False)\n",
    "\n",
    "# Print the covariance matrix\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the eigenvectors and eigenvalues of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "max_abs_idx = np.argmax(np.abs(eigenvectors), axis=0)\n",
    "signs = np.sign(eigenvectors[max_abs_idx, range(eigenvectors.shape[0])])\n",
    "eigenvectors = eigenvectors * signs[np.newaxis, :]\n",
    "eigenvectors = eigenvectors.T\n",
    "\n",
    "# Print the eigenvalues and eigenvectors\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[i, :]) for i in range(len(eigenvalues))]\n",
    "\n",
    "# Sort the tuples from the highest to the lowest based on eigenvalues magnitude\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For further usage\n",
    "eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n",
    "\n",
    "print(eig_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate explained variance and cumulative explained variance\n",
    "eig_vals_total = np.sum(eig_vals_sorted)\n",
    "explained_variance = [(i / eig_vals_total) * 100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))\n",
    "\n",
    "\n",
    "# Plotting cumulative explained variance\n",
    "num_components = len(explained_variance)\n",
    "plt.plot(np.arange(1, num_components + 1), cum_explained_variance, '-o')\n",
    "\n",
    "# Set x-axis tick labels explicitly\n",
    "custom_ticks = [0, 50, 100, 150, 200, 250, 300]\n",
    "plt.xticks(custom_ticks)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ef168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Over two components\n",
    "k = 130\n",
    "W = eig_vecs_sorted[:k, :]  # Projection matrix\n",
    "\n",
    "X_proj = X_training_scaled.dot(W.T)\n",
    "\n",
    "print(X_proj.shape)\n",
    "\n",
    "plt.scatter(X_proj[:, 0], X_proj[:, 1], c=y_training)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('2 components, captures {} of total variation'.format(cum_explained_variance[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a71129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA:\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components   \n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Standardize data \n",
    "        X = X.copy()\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.scale = np.std(X, axis=0)\n",
    "        X_std = (X - self.mean) / self.scale\n",
    "        \n",
    "        # Eigendecomposition of covariance matrix       \n",
    "        cov_mat = np.cov(X_std.T)\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_mat) \n",
    "        \n",
    "        # Adjusting the eigenvectors that are largest in absolute value to be positive    \n",
    "        max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "        signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "        eig_vecs = eig_vecs * signs[np.newaxis, :]\n",
    "        eig_vecs = eig_vecs.T\n",
    "       \n",
    "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i, :]) for i in range(len(eig_vals))]\n",
    "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "        eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n",
    "        \n",
    "        self.components = eig_vecs_sorted[:self.n_components, :]\n",
    "        \n",
    "        # Explained variance ratio\n",
    "        self.explained_variance_ratio = [i / np.sum(eig_vals) for i in eig_vals_sorted[:self.n_components]]\n",
    "        \n",
    "        self.cum_explained_variance = np.cumsum(self.explained_variance_ratio)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X_std = (X - self.mean) / self.scale\n",
    "        X_proj = X_std.dot(self.components.T)\n",
    "        \n",
    "        return X_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca = MyPCA(n_components=130).fit(X_training)\n",
    "\n",
    "#print('Components:\\n', my_pca.components)\n",
    "#print('Explained variance ratio:\\n', my_pca.explained_variance_ratio)\n",
    "#print('Cumulative explained variance:\\n', my_pca.cum_explained_variance)\n",
    "\n",
    "X_train_pca = my_pca.transform(X_training)\n",
    "X_test_pca = my_pca.transform(X_testing)\n",
    "print('Transformed train data shape:', X_train_pca.shape)\n",
    "print('Transformed test data shape:', X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870d987",
   "metadata": {},
   "source": [
    "## MultiClass Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5823c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train perceptron\n",
    "def initialize_weights(num_features):\n",
    "    return np.random.uniform(-1, 1, size=num_features + 1)\n",
    "\n",
    "def perceptron_train(X_train, y_train, iterations, learning_rate, num_classes):\n",
    "    num_features = X_train.shape[1]\n",
    "    weights = []\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        class_weights = initialize_weights(num_features)\n",
    "        for _ in range(iterations):\n",
    "            # Shuffle data for each epoch\n",
    "            shuffled_indices = np.random.permutation(len(X_train))\n",
    "            X_train_shuffled = X_train[shuffled_indices]\n",
    "            y_train_shuffled = y_train[shuffled_indices]\n",
    "            for features, label in zip(X_train_shuffled, y_train_shuffled):\n",
    "                features_with_bias = np.insert(features, 0, 1)\n",
    "                target = 1 if label == class_label else 0\n",
    "                prediction = 1 if np.dot(class_weights, features_with_bias) >= 0 else 0\n",
    "                error = target - prediction\n",
    "                class_weights += learning_rate * error * features_with_bias\n",
    "        weights.append(class_weights)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Training perceptron\n",
    "weights = perceptron_train(X_training, y_training, iterations=50, learning_rate=0.001, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208076a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "def predict(X_test, weights):\n",
    "    predictions = []\n",
    "    for sample in X_test:\n",
    "        scores = [np.dot(sample, class_weights[1:]) + class_weights[0] for class_weights in weights]\n",
    "        predicted_label = np.argmax(scores)\n",
    "        predictions.append(predicted_label)\n",
    "    return np.array(predictions)\n",
    "\n",
    "predicted_labels = predict(X_testing, weights)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = np.mean(predicted_labels == y_testing)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d743a2",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Euclidean distance\n",
    "def euclidean_dist(pointA, pointB):\n",
    "\n",
    "    distance = np.square(pointA - pointB) # (ai-bi)**2 for every point in the vectors\n",
    "    distance = np.sum(distance) # adds all values\n",
    "    distance = np.sqrt(distance)\n",
    "    return distance\n",
    "\n",
    "#Calculating distance of test point from all the training points\n",
    "def distance_from_all_training_points(test_point):\n",
    "#Returns- dist_array- Array holding distance values for all training data points\n",
    "    dist_array = np.array([])\n",
    "    for train_point in X_train_pca:\n",
    "        dist = euclidean_dist(test_point, train_point)\n",
    "        dist_array = np.append(dist_array, dist)\n",
    "    return dist_array\n",
    "\n",
    "#Implementing KNN Classification model\n",
    "def KNNClassifier(train_features, train_target, test_features, k = 5):\n",
    "  predictions = np.array([])\n",
    "  train_target = train_target.reshape(-1,1)\n",
    "  for test_point in test_features: # iterating through every test data point\n",
    "    dist_array = distance_from_all_training_points(test_point).reshape(-1, 1)\n",
    "    neighbors = np.concatenate((dist_array, train_target), axis=1)\n",
    "    neighbors_sorted = neighbors[neighbors[:, 0].argsort()] # sorts training points on the basis of distance\n",
    "    k_neighbors = neighbors_sorted[:k] # selects k-nearest neighbors\n",
    "    target_class = np.argmax(np.bincount(k_neighbors[:, 1].astype(int)))# selects label with highest frequency\n",
    "    predictions = np.append(predictions, target_class)\n",
    "  return predictions\n",
    "\n",
    "#running inference on test data\n",
    "test_predictions = KNNClassifier(X_train_pca, y_training, X_test_pca, k = 5)\n",
    "test_predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "#Model Evaluation\n",
    "def accuracy(y_test, y_preds):\n",
    "    total_correct = sum(1 for true, pred in zip(y_test, y_preds) if true == pred)\n",
    "    acc = total_correct / len(y_test)\n",
    "    return acc\n",
    "acc = accuracy(y_testing, test_predictions)\n",
    "print('Model accuracy (Scratch) = ', acc*100)\n",
    "print(\"Score:\\n\", classification_report(y_testing, test_predictions))\n",
    "\n",
    "import seaborn as sns\n",
    "k_values = list(range(1,20))\n",
    "accuracy_list = []\n",
    "for k in k_values:\n",
    "    test_predictions = KNNClassifier(X_train_pca, y_training, X_test_pca, k)\n",
    "    accuracy_list.append(accuracy(y_testing, test_predictions))\n",
    "sns.barplot(k_values, accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b4589",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d14b8",
   "metadata": {},
   "source": [
    "There are 10 classes and 130 features in pca reduced dataset. This accounts to more than 1300 likelihood probabilities. Hence, implementing this algorithm manually from scratch is cumbersome. All three Naive Bayes classifier types(Gaussian, Multinomial, Bernoulli) are implemented in the cells below using Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc044b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the classifiers\n",
    "clf1 = GaussianNB()\n",
    "clf2 = MultinomialNB()\n",
    "clf3 = BernoulliNB()\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Gaussian Naive Bayes Classifier\n",
    "try:\n",
    "  start = time.time()\n",
    "  clf1.fit(X_train_pca,y_training)\n",
    "  end = time.time()\n",
    "  print(f'time taken : {end - start}')\n",
    "\n",
    "  y_pred1 = clf1.predict(X_test_pca)\n",
    "  print(y_pred1.shape)\n",
    "  print(accuracy_score(y_testing,y_pred1))\n",
    "\n",
    "except Exception as e:\n",
    "  print(f'Some error occurred : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d57c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Multinomial Naive Bayes Classifier\n",
    "\n",
    "try:\n",
    "  start = time.time()\n",
    "  clf2.fit(X_train_pca,y_training)\n",
    "  end = time.time()\n",
    "  print(f'time taken : {end - start}')\n",
    "\n",
    "  y_pred2 = clf2.predict(X_test_pca)\n",
    "  print(y_pred2.shape)\n",
    "  print(accuracy_score(y_testing,y_pred2))\n",
    "\n",
    "except Exception as e:\n",
    "  print(f'Some error occurred : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = (X_train_pca < 0).sum().sum()\n",
    "\n",
    "print(f\"Total count of negative values : {negatives} Percentage : {(negatives / (50000*130))*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162d82f",
   "metadata": {},
   "source": [
    "Since a high fraction of values are negative, manipulating this dataset to handle negative values is not a good deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Bernoulli Naive Bayes Classifier\n",
    "\n",
    "try:\n",
    "  start = time.time()\n",
    "  clf3.fit(X_train_pca,y_training)\n",
    "  end = time.time()\n",
    "  print(f'time taken : {end - start}')\n",
    "\n",
    "  y_pred3 = clf3.predict(X_test_pca)\n",
    "  print(y_pred3.shape)\n",
    "  print(accuracy_score(y_testing,y_pred3))\n",
    "\n",
    "except Exception as e:\n",
    "  print(f'Some error occurred : {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c6742",
   "metadata": {},
   "source": [
    "### Verdict on Naive Bayes\n",
    "Given that the dataset is so feature and classes enriched, Multinomial NB could be an appropriate classifier but it does not take negative values which has resulted into an error as stated above. Moreover, there are 10 classes while Bernoulli is used for binary classification. Hence, Naive Bayes is not an ideal classifier for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
